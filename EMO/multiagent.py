import os
import asyncio
import json
import time
from dotenv import load_dotenv, find_dotenv
from flask_socketio import SocketIO
from google import genai
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.messages import TextMessage

# âœ… è¼‰å…¥ .env ä¸¦å•Ÿç”¨ Gemini åŸç”Ÿç”¨æ³•
dotenv_path = find_dotenv()
print(f"âœ… ç›®å‰ä½¿ç”¨çš„ .env è·¯å¾‘: {dotenv_path}")
load_dotenv(dotenv_path)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
print(GEMINI_API_KEY)

# âœ… ä½¿ç”¨ Gemini åŸç”Ÿ client
client = genai.Client(api_key=GEMINI_API_KEY)

# âœ… å°è£æˆç¬¦åˆ autogen agentchat çš„çµæ§‹
class GeminiChatCompletionClient:
    def __init__(self, model="gemini-1.5-flash-8b"):
        self.model = model
        self.model_info = {"vision": False}  # âœ… é¿å… autogen å‡ºéŒ¯

    async def create(self, messages, **kwargs):
        parts = []
        for m in messages:
            if hasattr(m, 'content'):
                parts.append(str(m.content))
            elif isinstance(m, dict) and 'content' in m:
                parts.append(str(m['content']))
        content = "\n".join(parts)
        response = client.models.generate_content(
            model=self.model,
            contents=content
        )
        #print("ğŸ“¨ Gemini å›æ‡‰å…§å®¹ï¼š", response)
        return type("Response", (), {
            "text": response.text,
            "content": response.text,
            "usage": {
                "prompt_tokens": {"value": 0},
                "completion_tokens": {"value": 0}
            }
        })

# âœ… åˆå§‹åŒ–å°è£å¾Œçš„ Gemini client
model_client = GeminiChatCompletionClient()

# âœ… å¯¦ç¾å…©å€‹ AI Agent äº’å‹•åˆ†æï¼Œä½¿ç”¨æ¨¡æ“¬æµå¼è¼¸å‡ºçš„æ–¹å¼
async def interactive_two_agent_analysis(socketio: SocketIO, dept_id, employee_data):
    # åŸºæœ¬çµ±è¨ˆæ•¸æ“š
    avg_satisfaction = employee_data["å“¡å·¥æ»¿æ„åº¦è©•åˆ†"].mean()
    min_satisfaction = employee_data["å“¡å·¥æ»¿æ„åº¦è©•åˆ†"].min()
    max_satisfaction = employee_data["å“¡å·¥æ»¿æ„åº¦è©•åˆ†"].max()
    
    # ä½æ»¿æ„åº¦å“¡å·¥æ¯”ä¾‹
    low_satisfaction_count = len(employee_data[employee_data["å“¡å·¥æ»¿æ„åº¦è©•åˆ†"] <= 2])
    low_satisfaction_percentage = (low_satisfaction_count / len(employee_data)) * 100
    
    # éš¨æ©Ÿé¸æ“‡ä»£è¡¨æ€§åé¥‹ä½œç‚ºæ¨£æœ¬
    if len(employee_data) > 5:
        sample = employee_data.sample(5)
    else:
        sample = employee_data
    
    sample_feedback = []
    for _, row in sample.iterrows():
        sample_feedback.append(f"å“¡å·¥ {row['å“¡å·¥ID']} (è©•åˆ† {row['å“¡å·¥æ»¿æ„åº¦è©•åˆ†']}): {row['è¿‘æœŸåé¥‹å…§å®¹']}")
    
    # ç¬¬ä¸€å€‹ Agentï¼ˆHR åˆ†æå°ˆå®¶ï¼‰çš„æç¤º
    analyst_prompt = f"""
    ä½œç‚ºäººåŠ›è³‡æºåˆ†æå°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹å“¡å·¥æ»¿æ„åº¦æ•¸æ“šé€²è¡Œè©³ç´°åˆ†æï¼š
    
    éƒ¨é–€: {dept_id}
    æ•´é«”æ»¿æ„åº¦åˆ†æ:
    - å¹³å‡æ»¿æ„åº¦è©•åˆ†: {avg_satisfaction:.2f}/5
    - æœ€ä½è©•åˆ†: {min_satisfaction}/5
    - æœ€é«˜è©•åˆ†: {max_satisfaction}/5
    - ä½æ»¿æ„åº¦å“¡å·¥æ¯”ä¾‹: {low_satisfaction_percentage:.1f}%
    
    å“¡å·¥åé¥‹æ¨£æœ¬:
    {json.dumps(sample_feedback, ensure_ascii=False, indent=2)}
    
    è«‹æä¾›è©³ç´°çš„åˆ†æï¼ŒåŒ…æ‹¬:
    1. é—œéµå•é¡Œè­˜åˆ¥
    2. æ»¿æ„åº¦åˆ†ä½ˆæ¨¡å¼åˆ†æ
    3. å“¡å·¥åé¥‹çš„ä¸»è¦ä¸»é¡Œå’Œè¶¨å‹¢
    
    è«‹ä¿æŒå°ˆæ¥­åˆ†æçš„èªæ°£ï¼Œä¸¦è¨»æ˜æ•¸æ“šæ”¯æŒçš„è§€é»ã€‚
    """
    
    socketio.emit('update', {
        'message': 'ğŸ¤– [HRåˆ†æå°ˆå®¶] æ­£åœ¨åˆ†æå“¡å·¥æ»¿æ„åº¦æ•¸æ“š...',
        'source': 'hr_analyst',
        'tag': 'analysis'
    })
    
    # æ¨¡æ“¬å»¶é²ï¼Œè®“ç”¨æˆ¶æ„ŸçŸ¥åˆ°å¯¦æ™‚åˆ†æéç¨‹
    await asyncio.sleep(1.5)
    
    # ç¬¬ä¸€å€‹ Agentï¼ˆHR åˆ†æå°ˆå®¶ï¼‰ç”Ÿæˆåˆ†æ
    try:
        response1 = client.models.generate_content(
            model="gemini-1.5-flash-8b",
            contents=analyst_prompt
        )
        
        analysis = response1.text.strip()
        
        # ç·©æ…¢åœ°ç™¼é€åˆ†æçµæœï¼Œæ¨¡æ“¬å¯¦æ™‚ç”Ÿæˆæ•ˆæœ
        segments = analysis.split('\n\n')
        for i, segment in enumerate(segments):
            if segment.strip():
                socketio.emit('update', {
                    'message': f"ğŸ¤– [HRåˆ†æå°ˆå®¶]ï¼š{segment}",
                    'source': "hr_analyst",
                    'tag': 'analysis'
                })
                # çŸ­æš«å»¶é²ï¼Œæ¨¡æ“¬æµå¼è¼¸å‡º
                await asyncio.sleep(0.8)
        
        # ç¬¬äºŒå€‹ Agentï¼ˆHR é¡§å•ï¼‰çš„æç¤ºï¼ŒåŒ…å«ç¬¬ä¸€å€‹ Agent çš„åˆ†æ
        consultant_prompt = f"""
        ä½œç‚ºäººåŠ›è³‡æºé¡§å•ï¼Œè«‹åŸºæ–¼åˆ†æå°ˆå®¶çš„ä»¥ä¸‹åˆ†æçµæœï¼Œæä¾›å…·é«”çš„æ”¹å–„å»ºè­°ï¼š
        
        éƒ¨é–€: {dept_id}
        åˆ†æå°ˆå®¶çš„ç™¼ç¾:
        {analysis}
        
        è«‹é‡å°ä¸Šè¿°åˆ†ææä¾›:
        1. å„ªå…ˆç´šæœ€é«˜çš„ä¸‰å€‹å•é¡Œ
        2. æ¯å€‹å•é¡Œçš„å…·é«”è§£æ±ºæ–¹æ¡ˆ
        3. çŸ­æœŸå’Œé•·æœŸçš„æ”¹å–„è¨ˆåŠƒ
        4. HR éƒ¨é–€æ‡‰è©²æ¡å–çš„è¡Œå‹•æ­¥é©Ÿ
        
        è«‹ä¿æŒå»ºè¨­æ€§å’Œå¯è¡Œæ€§ï¼Œä¸¦åœ¨å›ç­”æœ€å¾Œä»¥ã€Œæœ€çµ‚å»ºè­°ï¼šã€é–‹é ­ç¸½çµä½ çš„æ ¸å¿ƒå»ºè­°ã€‚
        """
        
        socketio.emit('update', {
            'message': 'ğŸ¤– [HRé¡§å•] æ­£åœ¨æ ¹æ“šåˆ†æçµæœç”Ÿæˆæ”¹å–„å»ºè­°...',
            'source': 'hr_consultant',
            'tag': 'analysis'
        })
        
        # çŸ­æš«å»¶é²ï¼Œå¢å¼·äº’å‹•æ„Ÿ
        await asyncio.sleep(1.5)
        
        # ç¬¬äºŒå€‹ Agentï¼ˆHR é¡§å•ï¼‰ç”Ÿæˆå»ºè­°
        response2 = client.models.generate_content(
            model="gemini-1.5-flash-8b", 
            contents=consultant_prompt
        )
        
        recommendations = response2.text.strip()
        
        # ç·©æ…¢åœ°ç™¼é€å»ºè­°çµæœï¼Œæ¨¡æ“¬å¯¦æ™‚ç”Ÿæˆæ•ˆæœ
        segments = recommendations.split('\n\n')
        for i, segment in enumerate(segments):
            if segment.strip():
                socketio.emit('update', {
                    'message': f"ğŸ¤– [HRé¡§å•]ï¼š{segment}",
                    'source': "hr_consultant",
                    'tag': 'analysis'
                })
                # çŸ­æš«å»¶é²ï¼Œæ¨¡æ“¬æµå¼è¼¸å‡º
                await asyncio.sleep(0.8)
        
        # æå–æœ€çµ‚å»ºè­°
        if "æœ€çµ‚å»ºè­°ï¼š" in recommendations:
            final_recommendation = recommendations.split("æœ€çµ‚å»ºè­°ï¼š")[-1].strip()
            socketio.emit('suggestions', {'suggestions': final_recommendation})
        else:
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°æœ€çµ‚å»ºè­°æ¨™è¨˜ï¼Œç”Ÿæˆä¸€å€‹ç°¡çŸ­ç¸½çµ
            summary_prompt = f"""
            è«‹ç¸½çµä»¥ä¸‹åˆ†æå’Œå»ºè­°çš„æ ¸å¿ƒè¦é»ï¼Œä¸¦æå‡ºæœ€é‡è¦çš„3é»è¡Œå‹•å»ºè­°ï¼š
            
            åˆ†æï¼š{analysis}
            
            å»ºè­°ï¼š{recommendations}
            """
            
            summary_response = client.models.generate_content(
                model="gemini-1.5-flash-8b",
                contents=summary_prompt
            )
            
            summary = summary_response.text.strip()
            socketio.emit('suggestions', {'suggestions': summary})
            
    except Exception as e:
        socketio.emit('update', {
            'message': f'âŒ åˆ†æéç¨‹å‡ºéŒ¯: {str(e)}',
            'tag': 'error'
        })

# âœ… ä¸»è¦åˆ†æå…¥å£é»å‡½æ•¸
async def run_multiagent_analysis(socketio: SocketIO, dept_id, employee_data):
    socketio.emit('update', {
        'message': 'ğŸ¤– ç³»çµ±ï¼šæ­£åœ¨å•Ÿå‹•HRåˆ†æå°ˆå®¶èˆ‡HRé¡§å•çš„å”ä½œ...',
        'tag': 'analysis'
    })
    try:
        # ä½¿ç”¨å…©å€‹äº’å‹•å¼ Agent é€²è¡Œåˆ†æ
        await interactive_two_agent_analysis(socketio, dept_id, employee_data)
    except Exception as e:
        socketio.emit('update', {
            'message': f'âŒ åˆ†æéç¨‹å‡ºç¾æœªé æœŸéŒ¯èª¤: {str(e)}',
            'tag': 'error'
        })